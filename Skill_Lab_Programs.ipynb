{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdHxnaSK22/be0/undA6fv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwalng2/sl/blob/main/Skill_Lab_Programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_tf_idf(documents, vocabulary):\n",
        "    N = len(documents)  # Number of documents\n",
        "    V = len(vocabulary)  # Size of the vocabulary\n",
        "\n",
        "    # Initialize TF matrix (N x V)\n",
        "    tf = np.zeros((N, V))\n",
        "\n",
        "    # Build term frequency (TF) matrix\n",
        "    for i, doc in enumerate(documents):\n",
        "        words = doc.lower().split()  # Split document into words\n",
        "        for word in words:\n",
        "            if word in vocabulary:\n",
        "                j = vocabulary.index(word)\n",
        "                tf[i, j] += 1\n",
        "        tf[i] = tf[i] / len(words)  # Normalize TF by document length\n",
        "\n",
        "    # Compute Document Frequency (DF)\n",
        "    df = np.zeros(V)\n",
        "    for j, term in enumerate(vocabulary):\n",
        "        df[j] = sum(1 for doc in documents if term in doc.lower().split())\n",
        "\n",
        "    # Compute Inverse Document Frequency (IDF)\n",
        "    idf = np.log(N / (df + 1))  # Add 1 to avoid division by zero\n",
        "\n",
        "    # Compute TF-IDF matrix\n",
        "    tf_idf = tf * idf  # Element-wise multiplication\n",
        "    return tf_idf\n",
        "\n",
        "# Example usage\n",
        "documents = [\n",
        "    \"cat sat on the mat\",\n",
        "    \"dog sat on the log\",\n",
        "    \"cat and dog played together\"\n",
        "]\n",
        "\n",
        "vocabulary = list(set(\" \".join(documents).lower().split()))\n",
        "tf_idf_matrix = compute_tf_idf(documents, vocabulary)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpnaICUvD-3v",
        "outputId": "d92f96ee-5ce0-430e-ab35-41c20251e5b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['the', 'dog', 'cat', 'log', 'together', 'and', 'on', 'mat', 'played', 'sat']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.08109302 0.         0.        ]\n",
            " [0.         0.         0.         0.08109302 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.08109302 0.08109302\n",
            "  0.         0.         0.08109302 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(sentence, n):\n",
        "    words = sentence.lower().split()  # Convert sentence to lowercase and split into words\n",
        "    ngrams = []\n",
        "    for i in range(len(words) - n + 1):  # Loop to create n-grams\n",
        "        ngram = tuple(words[i:i + n])  # Form a tuple of n consecutive words\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "# Example usage:\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "n = 3\n",
        "ngrams = generate_ngrams(sentence, n)\n",
        "\n",
        "print(f\"{n}-grams:\")\n",
        "for gram in ngrams:\n",
        "    print(gram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgPWmGQ-ECMV",
        "outputId": "b29c429a-b60c-4115-98d6-37185f1da4b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-grams:\n",
            "('the', 'quick', 'brown')\n",
            "('quick', 'brown', 'fox')\n",
            "('brown', 'fox', 'jumps')\n",
            "('fox', 'jumps', 'over')\n",
            "('jumps', 'over', 'the')\n",
            "('over', 'the', 'lazy')\n",
            "('the', 'lazy', 'dog.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_trigram_language_model(documents):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Initialize trigram counts\n",
        "    trigram_counts = defaultdict(int)\n",
        "    total_trigrams = 0\n",
        "\n",
        "    # Process each document to compute trigram counts\n",
        "    for doc in documents:\n",
        "        words = doc.lower().split()\n",
        "        for i in range(len(words) - 2):\n",
        "            trigram = tuple(words[i:i + 3])  # Extract a trigram\n",
        "            trigram_counts[trigram] += 1\n",
        "            total_trigrams += 1\n",
        "\n",
        "    # Compute trigram probabilities\n",
        "    trigram_probabilities = {}\n",
        "    for trigram, count in trigram_counts.items():\n",
        "        trigram_probabilities[trigram] = count / total_trigrams\n",
        "\n",
        "    return trigram_probabilities\n",
        "\n",
        "# Example usage:\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The quick blue fox jumps over the lazy cat\",\n",
        "    \"The lazy dog sleeps under the blue sky\"\n",
        "]\n",
        "\n",
        "trigram_model = compute_trigram_language_model(documents)\n",
        "\n",
        "print(\"Trigram Probabilities:\")\n",
        "for trigram, prob in trigram_model.items():\n",
        "    print(f\"{trigram}: {prob}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa5Pa0gFEOz3",
        "outputId": "df29eb6d-2ccd-493c-b911-fc431d6987c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram Probabilities:\n",
            "('the', 'quick', 'brown'): 0.05\n",
            "('quick', 'brown', 'fox'): 0.05\n",
            "('brown', 'fox', 'jumps'): 0.05\n",
            "('fox', 'jumps', 'over'): 0.1\n",
            "('jumps', 'over', 'the'): 0.1\n",
            "('over', 'the', 'lazy'): 0.1\n",
            "('the', 'lazy', 'dog'): 0.1\n",
            "('the', 'quick', 'blue'): 0.05\n",
            "('quick', 'blue', 'fox'): 0.05\n",
            "('blue', 'fox', 'jumps'): 0.05\n",
            "('the', 'lazy', 'cat'): 0.05\n",
            "('lazy', 'dog', 'sleeps'): 0.05\n",
            "('dog', 'sleeps', 'under'): 0.05\n",
            "('sleeps', 'under', 'the'): 0.05\n",
            "('under', 'the', 'blue'): 0.05\n",
            "('the', 'blue', 'sky'): 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(corpus, embedding_dim):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "\n",
        "    # Initialize embedding matrix with random values between 0 and 1\n",
        "    E = np.random.rand(V, embedding_dim)\n",
        "\n",
        "    # Create word to index mapping (already done in vocabulary)\n",
        "    word_to_index = vocabulary\n",
        "\n",
        "    # Define get_word_vector function\n",
        "    def get_word_vector(word):\n",
        "        word = word.lower()\n",
        "        if word in word_to_index:\n",
        "            idx = word_to_index[word]\n",
        "            return E[idx]\n",
        "        else:\n",
        "            return np.zeros(embedding_dim)\n",
        "\n",
        "    return E, vocabulary, get_word_vector\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "embedding_dim = 3\n",
        "\n",
        "# Create embedding matrix and supporting structures\n",
        "E, vocabulary, get_word_vector = create_embedding_matrix(corpus, embedding_dim)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Embedding Matrix E:\\n\", E)\n",
        "\n",
        "# Test get_word_vector\n",
        "word = \"learning\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n",
        "\n",
        "# Test with a word not in the vocabulary\n",
        "word = \"unknown\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTg9FHY-EZlK",
        "outputId": "3612fe53-421e-4383-e8cb-bcbadf0d1a61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "Embedding Matrix E:\n",
            " [[0.43424288 0.7058978  0.38601959]\n",
            " [0.86537809 0.37753575 0.63635562]\n",
            " [0.78650326 0.5444907  0.03288972]\n",
            " [0.21495396 0.60890938 0.38330596]\n",
            " [0.87393129 0.93176684 0.51772595]\n",
            " [0.64238237 0.57207472 0.31239986]\n",
            " [0.21528063 0.1502287  0.3641095 ]\n",
            " [0.52305078 0.2292095  0.01370816]]\n",
            "Embedding for 'learning': [0.21495396 0.60890938 0.38330596]\n",
            "Embedding for 'unknown': [0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix_with_pretrained(corpus, pretrained_embeddings, embedding_dim):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "\n",
        "    # Initialize embedding matrix\n",
        "    E = np.zeros((V, embedding_dim))\n",
        "\n",
        "    # Assign embeddings\n",
        "    for word, idx in vocabulary.items():\n",
        "        if word in pretrained_embeddings:\n",
        "            E[idx] = np.array(pretrained_embeddings[word])\n",
        "        else:\n",
        "            E[idx] = np.random.rand(embedding_dim)  # Random initialization\n",
        "\n",
        "    # Define get_word_vector function\n",
        "    def get_word_vector(word):\n",
        "        word = word.lower()\n",
        "        if word in vocabulary:\n",
        "            idx = vocabulary[word]\n",
        "            return E[idx]\n",
        "        else:\n",
        "            return np.zeros(embedding_dim)\n",
        "\n",
        "    return E, vocabulary, get_word_vector\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "pretrained_embeddings = {\n",
        "    \"machine\": [0.1, 0.2, 0.3],\n",
        "    \"learning\": [0.2, 0.3, 0.4],\n",
        "    \"amazing\": [0.3, 0.4, 0.5],\n",
        "    \"love\": [0.4, 0.5, 0.6]\n",
        "}\n",
        "\n",
        "embedding_dim = 3\n",
        "\n",
        "E, vocabulary, get_word_vector = create_embedding_matrix_with_pretrained(\n",
        "    corpus, pretrained_embeddings, embedding_dim\n",
        ")\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Embedding Matrix E:\\n\", E)\n",
        "\n",
        "# Test get_word_vector\n",
        "word = \"machine\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n",
        "\n",
        "word = \"i\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)  # Randomly initialized\n",
        "\n",
        "word = \"unknown\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)  # Returns zeros\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RPnTKBNErIV",
        "outputId": "25cf3786-47a2-4b9e-cf49-889b1a94c86f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "Embedding Matrix E:\n",
            " [[0.06588461 0.91788597 0.67036757]\n",
            " [0.4        0.5        0.6       ]\n",
            " [0.1        0.2        0.3       ]\n",
            " [0.2        0.3        0.4       ]\n",
            " [0.10699154 0.25140772 0.76108001]\n",
            " [0.3        0.4        0.5       ]\n",
            " [0.80456922 0.20992905 0.0958829 ]\n",
            " [0.08965483 0.58516625 0.82310121]]\n",
            "Embedding for 'machine': [0.1 0.2 0.3]\n",
            "Embedding for 'i': [0.06588461 0.91788597 0.67036757]\n",
            "Embedding for 'unknown': [0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_one_hot_encodings(corpus):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "\n",
        "    # Initialize one-hot encoding matrix\n",
        "    one_hot_encodings = {}\n",
        "    for word, idx in vocabulary.items():\n",
        "        one_hot_vector = np.zeros(V)\n",
        "        one_hot_vector[idx] = 1\n",
        "        one_hot_encodings[word] = one_hot_vector\n",
        "\n",
        "    return vocabulary, one_hot_encodings\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "vocabulary, one_hot_encodings = create_one_hot_encodings(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nOne-Hot Encodings:\")\n",
        "for word, one_hot_vector in one_hot_encodings.items():\n",
        "    print(f\"Word: '{word}' - One-Hot Vector: {one_hot_vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtj-v7FAE9ow",
        "outputId": "19de7305-3fcf-473a-e326-e58e54abceb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "One-Hot Encodings:\n",
            "Word: 'i' - One-Hot Vector: [1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Word: 'love' - One-Hot Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Word: 'machine' - One-Hot Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Word: 'learning' - One-Hot Vector: [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Word: 'is' - One-Hot Vector: [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Word: 'amazing' - One-Hot Vector: [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Word: 'new' - One-Hot Vector: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Word: 'things' - One-Hot Vector: [0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_skip_gram_pairs(sentences, window_size):\n",
        "    # Preprocessing: Build the vocabulary and word indices\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    # Generate skip-gram training pairs\n",
        "    training_pairs = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for i, target_word in enumerate(words):\n",
        "            # Define the context window\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(words), i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    context_word = words[j]\n",
        "                    training_pairs.append((target_word, context_word))\n",
        "\n",
        "    return vocabulary, training_pairs\n",
        "\n",
        "# Example usage:\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "window_size = 2\n",
        "vocabulary, training_pairs = generate_skip_gram_pairs(sentences, window_size)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nSkip-Gram Training Pairs:\")\n",
        "for pair in training_pairs:\n",
        "    print(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Hs1s4AFPLu",
        "outputId": "056106cb-1b74-4289-9545-61458441c9d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "Skip-Gram Training Pairs:\n",
            "('i', 'love')\n",
            "('i', 'machine')\n",
            "('love', 'i')\n",
            "('love', 'machine')\n",
            "('love', 'learning')\n",
            "('machine', 'i')\n",
            "('machine', 'love')\n",
            "('machine', 'learning')\n",
            "('learning', 'love')\n",
            "('learning', 'machine')\n",
            "('machine', 'learning')\n",
            "('machine', 'is')\n",
            "('learning', 'machine')\n",
            "('learning', 'is')\n",
            "('learning', 'amazing')\n",
            "('is', 'machine')\n",
            "('is', 'learning')\n",
            "('is', 'amazing')\n",
            "('amazing', 'learning')\n",
            "('amazing', 'is')\n",
            "('i', 'love')\n",
            "('i', 'learning')\n",
            "('love', 'i')\n",
            "('love', 'learning')\n",
            "('love', 'new')\n",
            "('learning', 'i')\n",
            "('learning', 'love')\n",
            "('learning', 'new')\n",
            "('learning', 'things')\n",
            "('new', 'love')\n",
            "('new', 'learning')\n",
            "('new', 'things')\n",
            "('things', 'learning')\n",
            "('things', 'new')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cbow_pairs(sentences, window_size):\n",
        "    # Preprocessing: Build the vocabulary and word indices\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    # Generate CBOW training pairs\n",
        "    training_pairs = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for i, target_word in enumerate(words):\n",
        "            # Define the context window\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(words), i + window_size + 1)\n",
        "            context_words = []\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    context_words.append(words[j])\n",
        "            if context_words:\n",
        "                training_pairs.append((tuple(context_words), target_word))\n",
        "\n",
        "    return vocabulary, training_pairs\n",
        "\n",
        "# Example usage:\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "window_size = 2\n",
        "vocabulary, training_pairs = generate_cbow_pairs(sentences, window_size)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nCBOW Training Pairs:\")\n",
        "for pair in training_pairs:\n",
        "    print(f\"Context: {pair[0]}, Target: {pair[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43zI6dzlFfNM",
        "outputId": "09976947-619b-4484-ebc1-a66c24b31a08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "CBOW Training Pairs:\n",
            "Context: ('love', 'machine'), Target: i\n",
            "Context: ('i', 'machine', 'learning'), Target: love\n",
            "Context: ('i', 'love', 'learning'), Target: machine\n",
            "Context: ('love', 'machine'), Target: learning\n",
            "Context: ('learning', 'is'), Target: machine\n",
            "Context: ('machine', 'is', 'amazing'), Target: learning\n",
            "Context: ('machine', 'learning', 'amazing'), Target: is\n",
            "Context: ('learning', 'is'), Target: amazing\n",
            "Context: ('love', 'learning'), Target: i\n",
            "Context: ('i', 'learning', 'new'), Target: love\n",
            "Context: ('i', 'love', 'new', 'things'), Target: learning\n",
            "Context: ('love', 'learning', 'things'), Target: new\n",
            "Context: ('learning', 'new'), Target: things\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rnn_forward(x, Wxh, Whh, Why, bh, by, h0):\n",
        "    h = h0\n",
        "    hs = []\n",
        "    ys = []\n",
        "    for t in range(len(x)):\n",
        "        xt = np.array([[x[t]]])  # Input at time t (make it a column vector)\n",
        "        h = np.tanh(np.dot(Whh, h) + np.dot(Wxh, xt) + bh)  # Hidden state\n",
        "        y = np.dot(Why, h) + by  # Output\n",
        "        hs.append(h)\n",
        "        ys.append(y)\n",
        "    return ys, hs\n",
        "\n",
        "# Example usage:\n",
        "# Input sequence\n",
        "x = [1, 2, 3]\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 1  # Since x is a sequence of numbers\n",
        "hidden_size = 4  # You can choose any size for the hidden state\n",
        "output_size = 1  # Output is a single number at each time step\n",
        "\n",
        "# Random initialization of weights and biases\n",
        "np.random.seed(0)  # For reproducibility\n",
        "Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Why = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "h0 = np.zeros((hidden_size, 1))\n",
        "\n",
        "# Run the RNN forward function\n",
        "ys, hs = rnn_forward(x, Wxh, Whh, Why, bh, by, h0)\n",
        "\n",
        "print(\"Outputs at each time step:\")\n",
        "for t, y in enumerate(ys):\n",
        "    print(f\"Time step {t+1}: y = {y.flatten()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAS9PsHkFtTe",
        "outputId": "73066d66-19bc-459d-b537-3cda9bd747cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs at each time step:\n",
            "Time step 1: y = [-0.00050584]\n",
            "Time step 2: y = [-0.00101643]\n",
            "Time step 3: y = [-0.00152624]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Compute the softmax of each element along the specified axis of x.\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))  # For numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "def self_attention(X, Wq, Wk, Wv):\n",
        "\n",
        "    # Compute Queries (Q), Keys (K), and Values (V)\n",
        "    Q = np.dot(X, Wq)  # Shape: (n, dout)\n",
        "    K = np.dot(X, Wk)  # Shape: (n, dout)\n",
        "    V = np.dot(X, Wv)  # Shape: (n, dout)\n",
        "\n",
        "    # Compute attention scores: Q * K.T, then scale by sqrt(dout)\n",
        "    d_k = Q.shape[1]  # dout\n",
        "    attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Shape: (n, n)\n",
        "\n",
        "    # Apply softmax to attention scores\n",
        "    attention_weights = softmax(attention_scores, axis=-1)  # Shape: (n, n)\n",
        "\n",
        "    # Compute final output: Attention weights * V\n",
        "    output = np.dot(attention_weights, V)  # Shape: (n, dout)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Example usage:\n",
        "np.random.seed(0)  # For reproducibility\n",
        "\n",
        "# Input matrix X (n=4 vectors, d=3 features per vector)\n",
        "X = np.random.rand(4, 3)  # Shape: (4, 3)\n",
        "\n",
        "# Learnable weight matrices Wq, Wk, Wv\n",
        "d = 3  # Input dimension\n",
        "dout = 2  # Output dimension\n",
        "Wq = np.random.rand(d, dout)  # Shape: (3, 2)\n",
        "Wk = np.random.rand(d, dout)  # Shape: (3, 2)\n",
        "Wv = np.random.rand(d, dout)  # Shape: (3, 2)\n",
        "\n",
        "# Call the self_attention function\n",
        "output = self_attention(X, Wq, Wk, Wv)\n",
        "\n",
        "print(\"Input Matrix X:\")\n",
        "print(X)\n",
        "print(\"\\nWeight Matrix Wq:\")\n",
        "print(Wq)\n",
        "print(\"\\nWeight Matrix Wk:\")\n",
        "print(Wk)\n",
        "print(\"\\nWeight Matrix Wv:\")\n",
        "print(Wv)\n",
        "print(\"\\nSelf-Attention Output:\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc-xbSbuGg6s",
        "outputId": "9f92359e-9c03-4888-f340-957fc9d861ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Matrix X:\n",
            "[[0.5488135  0.71518937 0.60276338]\n",
            " [0.54488318 0.4236548  0.64589411]\n",
            " [0.43758721 0.891773   0.96366276]\n",
            " [0.38344152 0.79172504 0.52889492]]\n",
            "\n",
            "Weight Matrix Wq:\n",
            "[[0.56804456 0.92559664]\n",
            " [0.07103606 0.0871293 ]\n",
            " [0.0202184  0.83261985]]\n",
            "\n",
            "Weight Matrix Wk:\n",
            "[[0.77815675 0.87001215]\n",
            " [0.97861834 0.79915856]\n",
            " [0.46147936 0.78052918]]\n",
            "\n",
            "Weight Matrix Wv:\n",
            "[[0.11827443 0.63992102]\n",
            " [0.14335329 0.94466892]\n",
            " [0.52184832 0.41466194]]\n",
            "\n",
            "Self-Attention Output:\n",
            "[[0.53569849 1.29450415]\n",
            " [0.53551973 1.29413435]\n",
            " [0.53849796 1.29925955]\n",
            " [0.53131543 1.28657939]]\n"
          ]
        }
      ]
    }
  ]
}